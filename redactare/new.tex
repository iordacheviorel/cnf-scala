\documentclass[12pt]{report}

%\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}
\usepackage[vlined]{algorithm2e}
\usepackage{dafny}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{scalerel,amssymb}
\usepackage{amsmath}
\usepackage{mathpartir}
\usepackage[inline]{enumitem}

\setlength{\parindent}{0mm}

\usepackage{paralist}
\usepackage{tabto}
\usepackage{comment}

\def\msquare{\mathord{\scalerel*{\Box}{gX}}}

%
\title{Verifying the Conversion into CNF in Dafny}
%
\author{Viorel Iordache \and
\c{S}tefan Ciob\^{a}c\u{a}}
%



\begin{document}

\maketitle     

\begin{abstract}

  We present two computer-verified implementations of the CNF
  conversion for propositional logic. The two implementations are
  fully verified: functional correctness and termination is
  machine-checked using the Dafny language. The first approach is
  based on repeatedly applying a set of equivalences and is often
  presented in logic textbooks. The second approach is based on
  Tseitin's transformation and is more efficient. We present the main
  ideas behind our formalization and we discuss the main difficulties
  in verifying the two algorithms.

\end{abstract}

\section{Introduction}

Several computer-checked solvers for the boolean satisfiability
problem have emerged relatively
recently~\cite{DBLP:journals/jar/Maric09,DBLP:conf/vmcai/OeSOC12,DBLP:journals/jar/BlanchetteFLW18,DBLP:conf/nfm/Fleury19}. Most
SAT solvers work with CNF-SAT, where the input formula is known to be
in conjunctive normal form. This is not a limitation, since efficient
algorithms to find the CNF of any formula are known. In this article,
we address the problem of finding the CNF of a formula and we verify
in the Dafny~\cite{DBLP:conf/icse/Leino04} language~\footnote{We
  assume some familiarity with a system such as
  Dafny~\cite{DBLP:conf/icse/Leino04} or Why3~\cite{why3}. We give a
  very brief overview of Dafny in Appendix~\ref{sec:dafny} for readers
  unfamiliar with Dafny.} two such algorithms.

The first approach that we verify is based on the standard textbook
algorithm of applying a series of equivalences from left to right as
long as possible. We work with the following nine equivalences:
%
\def \MathparLineskip {\lineskip=0.45cm}
\begin{mathpar}
  (1)\;\varphi_1 \Leftrightarrow \varphi_2 \equiv (\varphi_1
  \Rightarrow \varphi_2) \land (\varphi_2 \Rightarrow \varphi_1);
  \and
  (2)\;\varphi_2 \Rightarrow \varphi_1 \equiv \neg \varphi_1
   \lor \varphi_2
   \and
  (3)\; \varphi_1 \lor (\varphi_2 \land \varphi_3) \equiv
   (\varphi_1 \lor \varphi_2) \land (\varphi_1 \lor \varphi_3);
   \and
  (4)\; (\varphi_2 \land \varphi_3) \lor \varphi_1 \equiv
  (\varphi_2 \lor \varphi_1) \land (\varphi_3 \lor \varphi_1);
  \and
  (5)\; \varphi_1 \lor (\varphi_2 \lor \varphi_3) \equiv
  (\varphi_1 \lor \varphi_2) \lor \varphi_3;
  \and
  (6)\; \varphi_1 \land (\varphi_2 \land \varphi_3) \equiv
  (\varphi_1 \land \varphi_2) \land \varphi_3;
  \and
  (7)\; \neg (\varphi_1 \lor \varphi_2) \equiv \neg \varphi_1
  \land \neg \varphi_2;
  \and
  (8)\; \neg (\varphi_1 \land \varphi_2) \equiv \neg \varphi_1
  \lor \neg \varphi_2;
  \and
  (9)\; \neg \neg \varphi \equiv \varphi.
\end{mathpar}
%\def \MathparLineskip \MathparNormalpar

The first two equivalences remove implications and double
implications, equivalences three and four distribute disjunctions over
conjunctions, equivalences five and six associate parentheses in a
standard form and the last three are used to push negations towards
the leaves.

This approach has two advantages: it is simple and it does not
introduce new variables. However, the disadvantage is that certain
classes of formulae, such as
$ (x_1 \land x_1') \lor (x_2 \land x_2') \lor \ldots \lor (x_n \land
x_n')$ (\(n \geq 1 \)), lead to exponentially large CNFs.

\emph{First contribution.} We verify in Dafny that an algorithm based
on applying the nine equivalences above is functionally correct. The
most difficult part is to prove termination, for which we use a
carefully designed 5-tuple as a variant. To our knowledge, this is
incidentally the first proof (paper or computer-checked) of
termination of the nine rules. Indeed, in all logic textbooks that we
surveyed, termination is only proved for a certain strategy (first
applying rules 1 and 2, then finding a NNF using rules 7-9, etc.) of
applying the rules. The interest into this is of theoretical interest,
since other strategies (such as bringing the formula into NNF first)
are easier to prove.

The second approach that we verify is a so-called definitional
CNF~\cite{harrison} based on Tseitin's~\footnote{also spelled
  \emph{Tseytin}.}  transformation~\cite{Tseitin}. The idea is to
create fresh boolean variables for each subformula. Each such fresh
variable is constrained, by using carefully chosen clauses in the
resulting CNF, to be equivalent to its associated subformula. This
approach produces a CNF that is linear in the size of the given
formula, with the theoretical disadvantage that the CNF is only
equisatisfiable with the initial formula (not equivalent in general).

\emph{Second contribution}. We verify an implementation of Tseitin’s
transformation in Dafny. The main difficulty is to find the right
inductive invariant. There are also some technical difficulties with
the verification: our implementation pushes the prover to its limits
and requires carefully designed lemmas, helper predicates, and
assertions in order to verify successfully. For this approach,
termination is established by Dafny automatically, since the function
is recursive on the formula ADT. To our knowledge, this is the first
auto-active proof of Tseitin's transformation.

\emph{Paper structure}. In Section~\ref{sec:textbook} we present our
verified implementation of the textbook-based CNF transformation and
in Section~\ref{sec:tseitin} we present our verified implementation of
Tseitin's transformation. In Section~\ref{sec:related} we discuss
related work and in Section~\ref{sec:conclusion} we conclude and
discuss potential future work. There are two appendices: in
Appendix~\ref{sec:dafny} we give a very short overview of Dafny and in
Appendix~\ref{sec:development} we discuss the structure of our Dafny
development.

\emph{Repository}. The entire Dafny development is available at: 
\begin{center}\url{https://github.com/iordacheviorel/cnf-dafny}.\end{center}
In this paper, we only present what we believe are the most important
parts of the development, which are necessary in order to understand
our approach.

\section{The Textbook Conversion into CNF}
\label{sec:textbook}

We describe our verified implementation of the textbook CNF
transformation.

\subsection{Data Structures}

We represent boolean formulas as instances of the following algebraic
data type:

\begin{dafny}
datatype FormulaT = Var(val : int)  | Not(f1 : FormulaT)
| And(f1 : FormulaT, f2 : FormulaT) | Implies(f1 : FormulaT, f2 : FormulaT)
| Or(f1 : FormulaT, f2: FormulaT)   | DImplies(f1 :FormulaT, f2 : FormulaT)

\end{dafny}

We note that all standard logical connectives are available, that the
connectives have a fixed arity, and that variables are represented by
\emph{non-negative integers}. The fact that variables are represented
by non-negative integers is not encoded into the datatype; instead, we
follow the standard Dafny practice of checking this as a precondition
to all functions/methods computing with formulae by using a
predicate that we call \texttt{validFormulaT}.
%
% \begin{dafny}
% predicate validFormulaT(f : FormulaT) decreases f; {
%   match f {
%     case And(f1,f2)      => validFormulaT(f1) && validFormulaT(f2)
%     case Var(val)        => val >= 0
%     case Or(f1,f2)       => validFormulaT(f1) && validFormulaT(f2)
% [...]
%   }
% }
% \end{dafny}
%
% Note that the predicate above does not check validity in the
% semantical sense; Dafny standard practice is to define \texttt{valid}
% predicates to check class invariants and we borrow this naming scheme.
%
We often require to know not merely that a formula is syntactically
valid, but also to know that it contains at most \( n \)
variables. For this purpose, we use a predicate
\texttt{variablesUpTo}.% , which encompasses \texttt{validFormulaT}:
%
% \begin{dafny}
% predicate variablesUpTo(f : FormulaT, n : int) decreases f;
%   ensures variablesUpTo(f, n) == true ==> validFormulaT(f);
% {
%   match f {
%     case And(f1,f2)      => variablesUpTo(f1, n) && variablesUpTo(f2, n)
%     case Var(val)        => 0 <= val < n
%     case Or(f1,f2)       => variablesUpTo(f1, n) && variablesUpTo(f2, n)
% [...]
%   }
% }
% \end{dafny}
%
We define \texttt{truthValue} to compute the truth value of a formula in
a given assignment:

\begin{dafny}
function method truthValue(f: FormulaT, assignment : seq<bool>) : bool
  decreases f; requires variablesUpTo(f, |assignment|);
{ match f {
    case Var(val) => assignment[val]
    case Not(f1) => !truthValue(f1, assignment)
    case And(f1,f2) => truthValue(f1,assignment) && truthValue(f2,assignment)
    [...] } }
\end{dafny}

Truth assignments are represented as sequences of booleans
(\texttt{seq<bool>}) that have sufficiently many elements to account
for all variables in the formula, hence the \texttt{requires
  variablesUpTo(f, |assignment|)} precondition. As the truth value of
a formula is used both in the specification and in the implementation
of the algorithm, we declare it as a \texttt{function method}.

We define the predicate \texttt{equivalent} to check equivalence of
two formulae:

\begin{dafny}
predicate equivalent(f1 : FormulaT, f2 : FormulaT)
    requires validFormulaT(f1) && validFormulaT(f2);
{ forall tau : seq<bool> :: variablesUpTo(f1,|tau|) && variablesUpTo(f2,|tau|) 
      ==> truthValue(f1, tau) == truthValue(f2, tau) }
\end{dafny}

The predicate checks that the truth values of the two formulae are the
same in any (sufficiently large) truth assignment.

\subsection{Algorithm}

We implement the CNF conversion algorithm using three methods:

\( \bullet \) The method \texttt{applyAtTop} takes a formula and tries
to apply one of the nine equivalence rules \emph{at the root of the
  formula}. If it fails, the formula is returned unchanged.
 
\begin{dafny}
method applyAtTop(f:FormulaT, ghost orsAbvLft:int, ghost andsAbvLft:int)
  returns (r : FormulaT) decreases f;
    requires orsAbvLft >= 0 && andsAbvLft >= 0 && validFormulaT(f);
    ensures validFormulaT(r) && equivalent(f, r);
    ensures f == r ==> !f.Implies? && f == r ==> !f.DImplies?;
    ensures r == f || Utils.smaller(measure(r, orsAbvLft, andsAbvLft),
      measure(f, orsAbvLft, andsAbvLft));
{ match f {
    case DImplies(f1, f2) => { r := applyRule1(f, orsAbvLft, andsAbvLft); }
    case Implies(f1, f2)  => { r := applyRule2(f, orsAbvLft, andsAbvLft); }
    case Or(f1, f2) => { if (f2.And?) {
          r := applyRule3(f, orsAbvLft, andsAbvLft); } [...]
    } [...] } [...] }
\end{dafny}

We present the entire specification (pre- and post-conditions), but we
skip some implementation details (replaced by \texttt{[...]}). The two
ghost parameters are used in the termination proof, as discussed in
Section~\ref{sec:termination}. The first \texttt{ensures} clause is
used for functional correctness. The last \texttt{ensures} clauses are
used to prove termination of the main algorithm. In particular, the
function \texttt{measure} returns a tuple that acts as the variant of
the main algorithm.

We also define the methods \texttt{applyRule1}, \texttt{applyRule2},
\ldots, \texttt{applyRule9}, which apply one of the nine rules. We
present the first of these methods:

\begin{dafny}
method applyRule1(f : FormulaT, ghost orsAbvLft : int,
    ghost andsAbvLft : int)
  returns (r : FormulaT) requires validFormulaT(f) && f.DImplies?;
  requires orsAbvLft >= 0 && andsAbvLft >= 0;
  ensures validFormulaT(r) && equivalent(f, r);
  ensures weightOfAnds(r) <= weightOfAnds(f);
  ensures countDImplies(r) < countDImplies(f);
  ensures smaller(measure(r, orsAbvLft, andsAbvLft),
      measure(f, orsAbvLft, andsAbvLft));
{  var DImplies(f1, f2) := f;
   r := And(Implies(f1, f2), Implies(f2, f1));
   [...] }
\end{dafny}

Again, we show the entire specification, most of which is needed for
the termination proof. The missing part in the implementation, denoted
by \texttt{[...]}, are helper assertions and lemma calls that are
required to prove the postconditions.

\( \bullet \) The method \texttt{applyRule} takes a formula, traverses
its tree in preorder, and calls \texttt{applyAtTop} to transform the
first subformula where it is possible to do so at the root. Therefore,
the method \texttt{applyRule} makes exactly one effective call to
\texttt{applyAtTop}. We present the entire specification and the
implementation of one of the cases for \texttt{applyRule}:

\label{method:applyRule}
\begin{dafny}
method applyRule(f : FormulaT, ghost orsAbvLft : int, ghost andsAbvLft : int)
  returns (r : FormulaT) decreases f;
  requires validFormulaT(f) && orsAbvLft >= 0 && andsAbvLft >= 0;
  ensures validFormulaT(r) && equivalent(f, r);
  ensures r = f || Utils.smaller(measure(r,orsAbvLft ,andsAbvLft),
    measure(f,orsAbvLft ,andsAbvLft));
{ var res : FormulaT := applyAtTop(f, orsAbvLft, andsAbvLft);
  if (res != f) { return res; } else if (f.Or?) {
    var f1_step := applyRule(f.f1, orsAbvLft , andsAbvLft);
    if (f.f1 = f1_step) {
      var f2_step := applyRule(f.f2, orsAbvLft + 1, andsAbvLft);
      assert equivalent(f.f2, f2_step);
      assert equivalent(Or(f.f1, f.f2), Or(f.f1, f2_step));
      res := Or(f.f1, f2_step);
      if (weightOfAnds(f2_step) < weightOfAnds(f.f2)) {
        Rule3Or(f.f1, f.f2, f2_step); }
      return res;
    } else { [...] }
  } else if (f.And?) { [...] } else if (f.Not?) { [...] } }
\end{dafny}

Note again that the two ghost parameters are only used to help prove
termination of the main algorithm and that the pre- and
post-conditions are very similar to \texttt{applyRule}. Also note that
there are no cases for \texttt{f.Implies?} and \texttt{f.DImplies?}
since in these two cases \texttt{applyAtTop} is forced to make
progress. The lemma \texttt{Rule3Or} is used to propagate a
termination variant upwards in the tree of the formula and is
explained in Section~\ref{sec:termination}.

\( \bullet \) The main method in the algorithm is
\texttt{convertToCNF}. It takes a formula and calls \texttt{applyRule}
on it in a recursive loop until there are no more changes.

\begin{dafny}
method convertToCNF(f : FormulaT) returns (r : FormulaT)
  decreases weightOfAnds(f);/*3,4,7,8,9*/ decreases countDImplies(f);/*1*/
  decreases countImplies(f);/*2*/         decreases countOrPairs(f,0);/*5*/
  decreases countAndPairs(f, 0);/*6*/     requires validFormulaT(f);
  ensures validFormulaT(r) && equivalent(f, r);
{ var res := applyRule(f, 0, 0); assert equivalent(f, res);
  if(res != f) { r := convertToCNF(res);
                assert equivalent(res, r); [...]
  } else { r := res; } }
\end{dafny}

The main difficulty here is to prove the termination of this
fixed-point method. For this purpose, we use as a variant a 5-tuple
\texttt{measure}, %
% \texttt{h2}:
% \begin{dafny}
% function measure(f : FormulaT, h1:int, h2:int) : (int, int, int, int, int)
%   requires h1 >= 0 && h2 >= 0;
% {
%   (weightOfAnds(f), countDImplies(f), countImplies(f), 
%   countOrPairs(f, h1), countAndPairs(f, h2))
% }
% \end{dafny}
% Unfortunately, for technical reasons, it seems that Dafny rejects the
% use of the \texttt{measure} function in the \texttt{decreases}
% annotation:
% \begin{dafny}
% method convertToCNF(f : FormulaT) 
%   returns (r : FormulaT)
%   decreases measure(f, 0, 0);
%   [...]
% {
%   [...]
% }
% \end{dafny}
% \noindent and therefore
whose definition we unfold in the five \texttt{decreases} clauses of
\texttt{convertToCNF}. The numbers in the comments represent the
equivalences among the set of nine that ensure a strict decrease of
the particular element of the tuple.

\subsection{Proof of Termination}
\label{sec:termination}

In this section, we discuss in more depth the proof of termination 
which \emph{seems} intuitively easy:
%
\begin{enumerate*}
%
\item[(I1)] it \emph{seems} that the first two equivalences strictly
  decrease the number of double implications and implications,
  respectively;
%
\item[(I2)] it \emph{seems} that equivalences three and four strictly
  decrease the number of disjunctions that sit above conjunctions in
  the tree of the formula;
%
\item[(I3)] it \emph{seems} that rule five (resp. six) strictly decrease
  the number of \texttt{or}s just above and to the left of another
  \texttt{or} (resp. \texttt{and});
%
\item[(I4)] it \emph{seems} that rules 7-9 strictly decrease the number
  of negations above conjunctions and disjunctions.
%
\end{enumerate*}
%
The above intuition works when using a particular strategy to compute
a CNF: first, remove double implications; secondly, remove
implications; thirdly, compute the NNF, etc.

\emph{Difficulties}. Unfortunately, all of the above intuition breaks
when the rules can be applied in any order. There are two main
difficulties with the variant candidates above:
%
\begin{enumerate*}
%
\item[(D1)]\label{dif:d1} Equivalences one, three, and four are not
  right-linear; they might duplicate subformulae. Therefore, the
  variants suggested by intuitions I1, I2, I3, I4 (e.g., number of
  double implications) might actually increase when applying one of
  these equivalences. The apparent solution of counting the number of
  distinct subformulae rooted in a double implication instead of the
  number of double implications does not work either: after the
  initial duplication, the two subformulae might be transformed in
  different ways.
%
\item[(D2)] The variants suggested by intuitions I1-I4 do not in general
  extend homomorphically to the root of a formula when the
  transformation is performed in a proper subformula. Take for example
  the number of \texttt{and}s directly above and to the left of
  another \texttt{and} node (intuition I3). If we apply rule three in
  a context of the form \( \varphi \land \msquare \), we obtain
  \( \varphi \land ((\varphi_1 \lor \varphi_2) \land (\varphi_1 \lor
  \varphi_3)) \) from
  \( \varphi \land ( \varphi_1 \lor (\varphi_2 \land \varphi_3) ) \)
  and therefore our variant candidate actually increases at the root.
%
\end{enumerate*}

\emph{The main variant}. In order to prove termination, we rely
instead on a numerical interpretation of formulae that we call
\texttt{weightOfAnds}. It decreases strictly in equivalences 3, 4, 7,
8, and 9 and it does not increase in the other equivalences.

\begin{dafny}
function weightOfAnds(f : FormulaT) : (res : int)
  decreases f; ensures res >= 2;
{ match f {
    case Var(val)        => 2
    case Not(f1)         => pow(2, weightOfAnds(f1))
    case And(f1, f2)     => weightOfAnds(f1) + weightOfAnds(f2) + 1
    case Or(f1, f2)      => weightOfAnds(f1) * weightOfAnds(f2)
    case Implies(f1, f2) => pow(2, weightOfAnds(f1)) * weightOfAnds(f2)
    case DImplies(f1,f2) => (pow(2,weightOfAnds(f1)) * weightOfAnds(f2)) +
                    (pow(2,weightOfAnds(f2)) * weightOfAnds(f1)) + 1 } }
\end{dafny}

Intuitively (although we admit this is not a perfect intuition), this
function computes the number of conjunctions that a formula might have
when brought into CNF, hence the \texttt{+ 1} in the \texttt{And(f1,
  f2)} case. For disjunctions, intuitively the \texttt{Or} needs to be
distributed over all \texttt{And}s and therefore we use
multiplication. For negation, the number might increase exponentially
(negation ``turns'' \texttt{Or}s into \texttt{And}s and
vice-versa). The \texttt{pow} function is not builtin; it performs
exponentiation and is defined in the Dafny development along with
several helper lemmas.  For technical reasons, for leaves the counting
starts at \texttt{2} (\texttt{case Var(val) => 2}).

The cases for implication and double implication are handled as if an
implication \( \varphi_1 \Rightarrow \varphi_2 \) is just syntactic
sugar for \( \lnot \varphi_1 \lor \varphi_2 \) and therefore applying
equivalences 1 and 2 trivially does not change the value of
\texttt{weightOfAnds}. Equivalences 5 and 6 also do not change the
value of \texttt{weightOfAnds}, as the numerical interpretations of
\( \lor \) and of \( \land \) are associative. This numerical
interpretation is homomorphic, and therefore difficulty D2 is handled.


% \emph{The main variant strictly decreases}. For illustration purposes,
% we show why
% We prove in Dafny that \texttt{weightOfAnds} decreases strictly for
% equivalences 3, 4, 7, 8, and 9. The proof requires several helper
% lemmas for establishing some numerical results.
%
% Let
% $a = \texttt{weightOfAnds($\varphi_1$)}$,
% $b = \texttt{weightOfAnds($\varphi_2$)}$, and
% $c = \texttt{weightOfAnds($\varphi_3$)}$.
%
% \begin{comment}
% \begin{figure}[t!]
% \centering
% \begin{tikzpicture}[
%     tlabel/.style={pos=0.4,right=-1pt},
%     baseline=(current bounding box.center),
%     level 1/.style={sibling distance=25mm},
%     level 2/.style={sibling distance=15mm}    
%     ]
% 	\node [circle, draw]{$\lor$}
% 		child{node [circle, draw, label = left:{a}]{$\varphi_1$}}
% 		child{node [circle, draw]{$\land$}
% 			child{node [circle, draw, label = left:{b}]{$\varphi_2$}}
% 			child{node [circle, draw, label = left:{c}]{$\varphi_3$}}
% 		}
% ;
%   \end{tikzpicture}
%   $\equiv$ % the arrow between the pictures
%   \begin{tikzpicture}[
%     tlabel/.style={pos=0.4,right=-1pt},
%     baseline=(current bounding box.center),
%     level 1/.style={sibling distance=40mm},
%     level 2/.style={sibling distance=20mm}
%     ]
% 	\node [circle, draw]{$\land$}
% 		child{node [circle, draw]{$\lor$}
% 			child{node [circle, draw, label = left:{a}]{$\varphi_1$}}
% 			child{node [circle, draw, label = left:{b}]{$\varphi_2$}}
% 		}
% 		child{node [circle, draw]{$\lor$}
% 			child{node [circle, draw, label = left:{a}]{$\varphi_1$}}
% 			child{node [circle, draw, label = left:{c}]{$\varphi_3$}}
% 		}
% ;
%   \end{tikzpicture}
% \caption{\label{fig:eq3}Equivalence 3}
% \end{figure}
% \end{comment}
%
% Writing \texttt{w} for \texttt{weightOfAnds}, we have that the
% numerical interpretation of the right-hand side is strictly smaller
% than the numerical interpretation of the left-hand side:
%
% % TODO
% \begin{align*}
% 	\texttt{w}((\varphi_1 \lor \varphi_2) \land (\varphi_1 \lor \varphi_3)) &< \texttt{w}(\varphi_1 \lor (\varphi_2 \land \varphi_3)) & \qquad \hfill \mbox{iff} \\
% 	a * b + a * c + 1  &< a * (b + c + 1) & \qquad \hfill \mbox{iff} \\
% 	a * b + a * c + 1  &< a * b + a * c + a & \qquad \hfill \mbox{iff} \\
% 		1 &< a.
% \end{align*}
%
% This computation also explains why we require the numerical
% interpretation of variables to be strictly greater than $1$. For
% equivalence 3, Dafny establishes automatically that the numerical
% interpretation decreases strictly, without any help from the user. For
% the last three equivalences, we require helper lemmas. The following
% lemma proves this inequality for Equivalence 7:
%
% \begin{dafny}
% lemma Rule7Prop(f1 : FormulaT, f2 : FormulaT)
%   requires weightOfAnds(f1) >= 2 && weightOfAnds(f2) >= 2;
%   ensures weightOfAnds(And(Not(f1), Not(f2))) <
%       weightOfAnds(Not(Or(f1, f2)));
% {
%   assert weightOfAnds(And(Not(f1), Not(f2))) ==
%     pow(2, weightOfAnds(f1)) + pow(2, weightOfAnds(f2)) + 1;
%   assert weightOfAnds(Not(Or(f1, f2))) ==
%     pow(2, weightOfAnds(f1) * weightOfAnds(f2));
%   if (weightOfAnds(f1) >= weightOfAnds(f2)) {
%     sumpow_upper(weightOfAnds(f1), weightOfAnds(f2));
%     assert pow(2, weightOfAnds(f1)) + pow(2, weightOfAnds(f2)) + 1 <
%       pow(2, weightOfAnds(f1) * weightOfAnds(f2));
%     assert weightOfAnds(And(Not(f1), Not(f2))) 
%       < weightOfAnds(Not(Or(f1, f2)));
%   } else {
%     sumpow_upper(weightOfAnds(f2), weightOfAnds(f1));
%   }
% }
% \end{dafny}
%
% This numerical interpretation is homomorphic, and therefore difficulty
% D2 is handled.
% These proofs require a helper lemma such as
% \texttt{Rule3Or} below (used in method \texttt{applyRule} on
% Page~\pageref{method:applyRule}), which propagates the variant upwards
% an \texttt{Or} node:

% \begin{dafny}
% lemma Rule3Or(f1 : FormulaT, f2 : FormulaT, f3 : FormulaT)
%   requires weightOfAnds(f3) < weightOfAnds(f2);
%   requires weightOfAnds(f1) >= 2 && weightOfAnds(f2) >= 2;
%   requires weightOfAnds(f3) >= 2;
%   ensures weightOfAnds(Or(f1, f3)) < weightOfAnds(Or(f1, f2));
% {
%   assert weightOfAnds(Or(f1, f3)) == weightOfAnds(f1) * weightOfAnds(f3);
%   assert weightOfAnds(Or(f1, f2)) == weightOfAnds(f1) * weightOfAnds(f2);
%   lessthan_mult_right(weightOfAnds(f1),
%     weightOfAnds(f2), weightOfAnds(f3));
%   assert weightOfAnds(f1) * weightOfAnds(f3) <
%     weightOfAnds(f1) * weightOfAnds(f2);
% }
% \end{dafny}


\emph{Secondary variants}. The main variant establishes termination of
rules 3, 4, 7, 8, and 9. To establish termination of the entire
system, we require 4 more secondary variants. The second and third
elements of the tuple (\texttt{countDImplies(f)} and
\texttt{countImplies(f)}) unsurprisingly count the number of double
implications and implications in a formula, respectively. These
decrease strictly when applying equivalences 1 and 2, respectively.

More interestingly, we discuss the last two components:
\texttt{countOrPairs(f, 0)} and \texttt{countAndPairs(f, 0)}. These
are used to establish termination of the rules for associating
parentheses to the left (equivalences 5 and 6). As explained in
difficulty D2 above, simply counting the number of \texttt{And} nodes
directly above and to the left of another \texttt{And} node does not
work, since the inequality required of such a variant does not
propagate from a subformula towards the root. Therefore, we count
instead the number of pairs of nodes labeled \texttt{And} such that
one is a (possibly indirect) ancestor of the other towards the
left. We display \texttt{countOrPairs}, as the other function is
similar:

\begin{dafny}
function countOrPairs(f : FormulaT, orsAbvLft : int) : (res : int)
  decreases f; requires orsAbvLft >= 0; ensures res >= 0;
{ match f {
    case Or(f11,f12) => countOrPairs(f11, orsAbvLft) +
      countOrPairs(f12, orsAbvLft + 1) + orsAbvLft // note the ``+ 1''
    case Var(val) => 0 
    case And(f11,f12) => countOrPairs(f11, orsAbvLft) +
      countOrPairs(f12, orsAbvLft)
    [...] } }
\end{dafny}

The helper parameter represents the number of \texttt{Or}s above and
to the left of the current subformula and should therefore be \( 0 \)
in the initial call. This variant propagates as required.

In order to implement this variant, we require to keep track in the
\texttt{applyRule} method of the number of \texttt{Or}s and
\texttt{And}s that are possibly indirect ancestors towards the left of
the current subformula. These numbers account for the two ghost
parameters \texttt{orsAbvLft} and \texttt{andsAbvLft} of the
\texttt{applyRule} and \texttt{applyAtTop} methods mentioned above and
left for the current subsection. Initially, when \texttt{applyRule} is
called in \texttt{convertToCNF}, both are initialized to \( 0 \).

The first three components of the variant do not commute, but the last
two could be interchanged without affecting the termination proof.

\section{The Tseitin Conversion}
\label{sec:tseitin}

In this section, we discuss our formalization of the Tseitin
conversion into CNF~\cite{Tseitin}, also called definitional
CNF~\cite{harrison}. We first briefly recall the Tseitin
transformation via a short example.


The guarantee offered by this transformation is that the resulting
formula is equisatisfiable to the initial formula, as opposed to
equivalent for the textbook transformation discussed in
Section~\ref{sec:textbook}. The advantage is that the size is at most
O(1) times bigger than the initial formula.

\subsection{Data Structures}

In verifying the definitional CNF transformation, we use the same data
type \texttt{FormulaT} for the input formula as in the previous
formalization. However, for the output formula, we choose to represent
literals as integers, clauses as sequences of integers and CNF
formulae as sequences of clauses, similar to the well-known DIMACS
format:

\begin{dafny}
predicate method validLiteral(lit : int) { lit <= -1 || lit >= 1 }
predicate validClause(clause : seq<int>) {
  forall lit :: lit in clause ==> validLiteral(lit) }
predicate validCnfFormula(f : seq<seq<int> >) {
  forall clause : seq<int> :: clause in f ==> validClause(clause) }
\end{dafny}

Note that \texttt{valid} in the predicates above reflects standard
Dafny use rather than semantical validity in the logical sense. As
variables are represented by non-negative integers, we consistently
use the following function (methods) to convert between variables and
literals:

\begin{dafny}
predicate validVariable(v : int) { v >= 0 }
function method posVarToLit(v : int) : int
  requires validVariable(v);
  ensures posVarToLit(v) >= 1 && validLiteral(posVarToLit(v)); 
{ v + 1 }
function method negVarToLit(v : int) : int
  requires validVariable(v);
  ensures negVarToLit(v) <= -1 && validLiteral(negVarToLit(v)); 
{ (-v) - 1 }
function method litToVar(l : int) : int
  requires validLiteral(l);
{ if (l <= -1) then (-l) - 1 else l - 1 }
\end{dafny}

We represent assignments as sequences of booleans, just like in the
textbook transformation. To compute truth values, we use the following
predicates:

\begin{dafny}
predicate truthValueLiteral(lit : int, tau : seq<bool>)
    requires validLiteral(lit) && variablesUpToLiteral(lit, |tau|);
{ if lit < 0 then !tau[litToVar(lit)] else tau[litToVar(lit)] }
predicate truthValueClause(clause : seq<int>, tau : seq<bool>)
  requires validClause(clause) && variablesUpToClause(clause, |tau|);
{ exists lit :: lit in clause && truthValueLiteral(lit, tau) }
predicate truthValueCnfFormula(rf : seq<seq<int>>, tau : seq<bool>)
  requires validCnfFormula(rf) && variablesUpToCnfFormula(rf, |tau|);
{ forall clause | clause in rf :: truthValueClause(clause, tau) }
\end{dafny}

The predicates \texttt{variablesUpTo*} (with
\(\texttt{*} \in \{ \texttt{CnfFormula}, \allowbreak \texttt{Clause},
\texttt{Literal} \} \)), check that the assignment is sufficiently
large to account for all variables occurring in the formulae. We
consistently use the following convention: program variables such as
\texttt{f}, \texttt{f1}, \texttt{f2} stand for formulae of type
\texttt{FormulaT} and program variables such \texttt{rf}, \texttt{rf1}
stand for \emph{resulting formulae} of type \texttt{seq<seq<int> >}.

As the guarantee of the Tseitin transformation is equisatisfiability
between the initial formula and the resulting formula, we model this
by using the following predicates:

\begin{dafny}
predicate satisfiable(f : FormulaT) requires validFormulaT(f);
{ exists tau | |tau| == maxVar(f) :: truthValue(f, tau) }
predicate satisfiableCnfFormula(rf:seq<seq<int> >) requires validCnf[...](rf);
{ exists tau | |tau| == maxVarCnfFormula(rf) :: truthValueCnfFormula(rf, tau) }
predicate equiSatisfiable(f : FormulaT, rf : seq<seq<int> >)
    requires validFormulaT(f); requires validCnfFormula(rf);
{ satisfiable(f) <==> satisfiableCnfFormula(rf) }
\end{dafny}

The function methods \texttt{maxVar*} compute the maximum natural
number that represents a variable inside the formulae, plus
one. Therefore an assignment of size \texttt{maxVar*} is sufficiently
large to compute the truth value.% :
% \begin{dafny}
% function method maxVarCnfFormula(rf : seq<seq<int> >) : int
%   requires validCnfFormula(rf);
%   ensures maxVarCnfFormula(rf) >= 0;
%   ensures forall clause | clause in rf ::
%     variablesUpToClause(clause, maxVarCnfFormula(rf));
%   ensures variablesUpToCnfFormula(rf, maxVarCnfFormula(rf));
% {
%   if |rf| == 0 then
%     [...];
%     0
%   else
%     var result := Utils.max(maxVarClause(rf[0]),
%       maxVarCnfFormula(rf[1..]));
%     [...];
%     result
% }

% function method maxVar(f : FormulaT) : int
%   requires validFormulaT(f);
%   ensures variablesUpTo(f, maxVar(f)) && maxVar(f) >= 0;
% {
%     [...]
% }
% \end{dafny}

\subsection{The Algorithm}

The entry point to the algorithm is the method \texttt{tseitin}.

\begin{dafny}
method tseitin(f : FormulaT) returns (result : seq<seq<int> >)
  requires validFormulaT(f);
  ensures validCnfFormula(result) && equiSatisfiable(f, result);
{ var n := maxVar(f); var v : int; var end : int; var rf : seq<seq<int> >;
  rf, v, end := tseitinCnf(f, n, n);
  result := rf + [[posVarToLit(v)]]; [...] }
\end{dafny}

The guarantee is that the resulting CNF formula is equisatisfiable to the
initial one. The main work is performed by the method
\texttt{tseitinCnf}, which traverses the input formula recursively and
adds the right clauses to the result:

\begin{dafny}
method tseitinCnf(f : FormulaT, n : int, start : int)
  returns (rf : seq<seq<int> >, v : int, end : int)
  requires variablesUpTo(f, n) && start >= n >= 0;
  ensures valid(f, rf, v, n, start, end);
  ensures tseitinSameValue(f, rf, v, n, start, end);
  ensures tseitinCanExtend(f, rf, v, n, start, end);
{ match f {
    case Or(f1, f2)        => {
      var rf1 : seq<seq<int> >; var rf2 : seq<seq<int> >;
      var v1 : int; var v2 : int;
      var mid : int;
      rf1, v1, mid := tseitinCnf(f1, n, start);
      rf2, v2, v := tseitinCnf(f2, n, mid);
      end := v + 1;
      rf := rf1 + rf2 + orClauses(v1, v2, v);
      proveCanExtendOr(f1,rf1,v1, f2, rf2, v2, n, start, mid, v, end, rf);
      proveSameValueOr(f1,rf1,v1, f2, rf2, v2, n, start, mid, v, end, rf);
    }
    case And(f1, f2)      => [...]  case Implies(f1, f2)  => [...]
    case DImplies(f1, f2) => [...]  case Not(f1) => [...]
    case Var(val) => [...]
} }
\end{dafny}

The method \texttt{tseitinCnf} takes as input:
%
\begin{enumerate*}
%
\item a formula \texttt{f} to transform into CNF, which might be a
  subformula of the initial formula given to \texttt{tseitin};
%
\item a natural number \texttt{n} with the meaning that all variables
  in the initial formula are between \( 0 \) and \( n - 1 \);
%
\item a natural number \texttt{start}, with the meaning that the
  variables \texttt{start}, \texttt{start + 1}, \texttt{start + 2},
  \ldots are not used and can be safely used as fresh variables by the
  method.
%
\end{enumerate*}
%
Variables between \texttt{n} (inclusively) and \texttt{start}
(exclusively) might have been used for some other subformulae.

The method \texttt{tseitinCnf} returns as output:
%
\begin{enumerate*}
%
\item A set of clauses \texttt{rf} encoding that the freshly chosen
  variables are equivalent to the corresponding subformulae;
%
\item A variable \texttt{v} that corresponds to the input formula
  \texttt{f};
%
\item A number \texttt{end} with the meaning that the recursive call
  used fresh variables between \texttt{start} (inclusively) and
  \texttt{end} (exclusively) and therefore the variables \texttt{end},
  \texttt{end + 1}, \texttt{end + 2}, \ldots can be used safely as
  fresh after the call is finished.
%
\end{enumerate*}
%
The predicate \texttt{valid} is used to account for the validity of
the entire state of the algorithm:

\begin{dafny}
predicate valid(f : FormulaT, rf : seq<seq<int> >, v : int,
  n : int, start : int, end : int)
{ 0 <= n <= start <= end && variablesUpTo(f, n) && validCnfFormula(rf) &&
  validVariable(v) && variableInInterval(v, n, start, end) &&
  variablesInInterval(rf, n, start, end) }
\end{dafny}

The predicate \texttt{variablesInInterval(rf, n, start, end)} checks
that \texttt{rf} uses only the initial variables (between \( 0 \) and
\( n - 1 \) ) and fresh variables between \texttt{start} (inclusively)
and \texttt{end} (exclusively).

We discuss in more detail the implementation of the \texttt{Or} case
in \texttt{tseitinCnf} presented above. Note how the recursive call on
\texttt{f1} uses fresh variables between \texttt{start} (inclusively)
and \texttt{mid} (exclusively), while the recursive call on
\texttt{f2} uses fresh variables between \texttt{mid} (inclusively)
and \texttt{v} (exclusively). The variable \texttt{v} is therefore
used as the fresh variable corresponding to the entire formula
\texttt{f} = \texttt{Or(f1, f2)}. The final set of clauses is then the
union of \texttt{rf1} (set of clauses corresponding to \texttt{f1}),
\texttt{rf2} (set of clauses corresponding to \texttt{f2}) and
\texttt{orClauses(v1, v2, v)}, which encodes that \texttt{v} should be
equivalent to \texttt{Or(v1, v2)}:

\begin{dafny}
function method orClauses(v1 : int, v2 : int, v : int) : seq<seq<int> >
  requires validVariable(v1) && validVariable(v2) && validVariable(v);
{ [[negVarToLit(v), posVarToLit(v1), posVarToLit(v2)],
    [negVarToLit(v1), posVarToLit(v)], [negVarToLit(v2), posVarToLit(v)]] }
\end{dafny}
  
\subsection{The Proof}

The main difficulty in verifying the algorithm is coming up with the
right invariants. Assuming that \texttt{tseitinCnf(f, n, start)}
returns \texttt{rf, v, end}, we find that the following two invariants
explain the functional correctness of the algorithm:
%
\begin{enumerate*}
%
\item any truth assignment \texttt{tau} to the initial \texttt{n}
  variables can be extended (uniquely) to a truth assignment
  \texttt{tau'} that makes \texttt{rf} true and such that the value of
  \texttt{f} in \texttt{tau} is the same as the value of \texttt{v} in
  \texttt{tau'};
%
\item vice-versa, any truth assignment to all \texttt{end} variables
  that makes \texttt{rf} true also makes \texttt{v} and \texttt{f}
  have the same truth value.
%
\end{enumerate*}
%
We formalize the two invariants above in the predicates
\texttt{tseitinCanExtend} and \texttt{tseitinSameValue}:

\begin{dafny}
predicate tseitinCanExtend(f : FormulaT, rf : seq<seq<int> >,
    v : int, n : int, start : int, end : int)
  requires valid(f, rf, v, n, start, end);
{ forall tau : seq<bool> | |tau| == n :: canExtend(tau,f,rf,v,n,start,end) }

predicate canExtend(tau : seq<bool>, f : FormulaT, rf : seq<seq<int> >, 
    v : int, n : int, start : int, end : int)
  requires |tau| == n && valid(f, rf, v, n, start, end);
{ exists tau' : seq<bool> | tau <= tau' && |tau'| == end ::
    truthValueCnfFormula(rf, tau') && truthValue(f, tau) == 
    truthValueLiteral(posVarToLit(v), tau') }

predicate tseitinSameValue(f : FormulaT, rf : seq<seq<int> >,
    v : int, n : int, start : int, end : int)
  requires valid(f, rf, v, n, start, end);
{ forall tau : seq<bool> | |tau| >= end && truthValueCnfFormula(rf, tau) ::
    [...] truthValueLiteral(posVarToLit(v), tau) == truthValue(f, tau) }
\end{dafny}

We find that because \texttt{tseitinCanExtend} is of the form
\( \forall \_ . \exists \_ . \_ \) (nested quantifiers), it is useful
for verification performance to give a name to the
\( \exists \_ . \_ \) part, hence the predicate \texttt{canExtend}.
Dafny cannot prove the two predicates automatically, and therefore we
design helper lemma for each of the two invariants and for each of the
cases (\texttt{Or}, \texttt{And}, \texttt{Not}, \ldots).%  We present
% the proofs for the \texttt{Or} case, and the others are similar.
%
% The first of the two invariants is the most difficult to prove:
%
% \begin{dafny}
% lemma proveCanExtendOr( f1 : FormulaT, rf1 : seq<seq<int> >, v1 : int,
%     f2 : FormulaT, rf2 : seq<seq<int> >, v2 : int, n : int, start : int,
%     mid : int, v : int, end : int, rf : seq<seq<int> >)
%   requires 0 <= n <= start <= mid <= v;
%   requires valid(f1, rf1, v1, n, start, mid);
%   requires tseitinCanExtend(f1, rf1, v1, n, start, mid);
%   requires valid(f2, rf2, v2, n, mid, v);
%   requires tseitinCanExtend(f2, rf2, v2, n, mid, v);
%   requires rf == rf1 + rf2 + orClauses(v1, v2, v) && end == v + 1;
%   ensures valid(Or(f1, f2), rf, v, n, start, end);
%   ensures tseitinCanExtend(Or(f1, f2), rf, v, n, start, end);
% {
%   assert valid(Or(f1, f2), rf, v, n, start, end);
%   forall tau : seq<bool> | |tau| == n ensures canExtend(tau, Or(f1, f2),
%     rf, v, n, start, end);
%   {
%     ghost var tau1 :| tau <= tau1 && |tau1| == mid &&
%       truthValueCnfFormula(rf1, tau1) && 
%       truthValue(f1, tau) == truthValueLiteral(posVarToLit(v1), tau1);
%     ghost var tau2 :| tau <= tau2 && |tau2| == v &&
%       truthValueCnfFormula(rf2, tau2) &&
%       truthValue(f2, tau) == truthValueLiteral(posVarToLit(v2), tau2);
%     ghost var tau' := combine(tau, tau1, tau2, n, start, mid, v,
%         truthValue(Or(f1, f2), tau));
%
%     assert agree(tau', tau, 0, n);
%     assert agree(tau', tau1, start, mid);
%     assert agree(tau', tau2, mid, v);
%     [...]
%
%     lemmaOrClauses(v1, v2, v, tau');
%     assert canExtend(tau, Or(f1, f2), rf, v, n, start, end);
%   }
% }
% \end{dafny}
%
The main idea behind these proof is to combine two assignments
\texttt{tau1} and \texttt{tau2}, which necessarily agree on the first
\( n \) variables (the variables in the initial formula), into a
single assignment \texttt{tau'}. This is possible since the
\emph{interesting} assignments in \texttt{tau1} range from
\texttt{start} to \texttt{mid} and the \emph{interesting} assignments
in \texttt{tau2} range from \texttt{mid} to \texttt{v}; that is they
are disjoint. We ellide the computer-checked proof for space reasons.
% We elide 25
% lines of proof (replaced by \texttt{[...]})  that is used to properly
% prove that truth values of appropriate items transfer between
% \texttt{tau1} (resp. \texttt{tau2}) and \texttt{tau'}.

% An interesting proof technique that we use here is to reflect the
% meaning of the clauses returned by \texttt{orClauses} into the Dafny
% meta-language by using the following lemma:

% \begin{dafny}
% lemma lemmaOrClauses(v1 : int, v2 : int, v : int, tau : seq<bool>)
%     requires validVariable(v1) && validVariable(v2) && validVariable(v);
%     requires |tau| > v && |tau| > v1 && |tau| > v2;
%     ensures truthValueCnfFormula( orClauses(v1, v2, v), tau) <==>
%         ((truthValueLiteral(posVarToLit(v1), tau) ||
%         truthValueLiteral(posVarToLit(v2), tau)) <==>
%         truthValueLiteral(posVarToLit(v), tau));
% {
%     [...]
% }
% \end{dafny}

% The second invariant has a smaller proof for the \texttt{Or} case:

% \begin{dafny}
% lemma proveSameValueOr( f1 : FormulaT, rf1 : seq<seq<int> >, v1 : int,
%   f2 : FormulaT, rf2 : seq<seq<int> >, v2 : int, n : int, start : int, 
%   mid : int, v : int, end : int, rf : seq<seq<int> >)
%   requires 0 <= n <= start <= mid <= v;
%   requires valid(f1, rf1, v1, n, start, mid);
%   requires tseitinSameValue(f1, rf1, v1,
%       n, start, mid);
%   requires valid(f2, rf2, v2, n, mid, v);
%   requires tseitinSameValue(f2, rf2, v2,
%       n, mid, v);
%   requires rf == rf1 + rf2 + orClauses(v1, v2, v);
%   requires end == v + 1;
%   ensures valid(Or(f1, f2), rf, v, n, start, end);
%   ensures tseitinSameValue(Or(f1, f2), rf, v,
%       n, start, end);
% {
%   assert valid(Or(f1, f2), rf, v, n, start, end);
%   forall tau : seq<bool> | |tau| >= end && truthValueCnfFormula(rf, tau)
%     ensures truthValueLiteral(posVarToLit(v), tau) ==
%       truthValue(Or(f1, f2), tau)
%   {
%     [...]
%     lemmaOrClauses(v1, v2, v, tau);
      
%     assert truthValueLiteral(posVarToLit(v), tau) ==
%       truthValue(Or(f1, f2), tau);
%   }
% }
% \end{dafny}

% We elide 6 lines of proof. Note that we use the same proof technique
% of reflecting the meaning of \texttt{orClauses} into the Dafny
% meta-language.
  
\section{Related Work}
\label{sec:related}

The work closest to ours is by Barroso et
al.~\cite{DBLP:journals/corr/abs-2003-05081}, who verify a CNF
transformation for propositional logic in the Why3~\cite{why3}
verification platform. They use the textbook approach, but they rely
on a particular strategy (first, remove implication, then: compute the
negation normal form, etc.) This makes their proof much simpler,
especially w.r.t. termination. One theoretical difference is that they
model truth assignments as functions from variables to truth values
and therefore their specification is closer to the mathematical
treatment of logic (we instead model truth assignment as finite
sequences, but we ensure they are sufficiently large for the context
in which they are used). Barroso et al. emphasize the verification of
continuation-passing style of the CNF transformation, which is out of
the scope of our paper.

Michaelis and Nipkow~\cite{DBLP:conf/types/MichaelisN17} mechanize and
prove Tseitin's transformation in Isabelle/HOL as part of the
formalization~\cite{Propositional_Proof_Systems-AFP} of a series of
propositional proof systems. The implementation is functional, based
on first generating fresh names for all distinct subformulae and then
adding the corresponding clauses for each internal node of the input
formula. The fact that the fresh names are generated at the very
beginning seems to make the proof simpler. As the emphasis is placed
on metatheoretical considerations, efficiency is not the main
concern. In our Dafny approach, the implementation is more efficient
and is compositional: each subformula is recursively translated into a
set of clauses.

G\"{a}her and Kunze~\cite{gaher_et_al:LIPIcs.ITP.2021.20} implement
and verify Tsetin's transformation in the Coq proof assistant as part
of the proof of the Cook-Levin theorem. The algorithm is implemented
as a fixpoint (terminating, pure, recursive function) in the
functional language of the Coq proof assistant. The function is very
similar to our implementation: it takes a subformula and a natural
starting from which fresh identifiers can be chosen. It returns the
set of clauses for the subformula, the new variable associated to the
subformula and a new number to be used for freshness. The inductive
invariant \texttt{tseytin\textunderscore formula\textunderscore repr}
used for the proof is also very similar to what we have independently
found. Since the implementations are in very different proof
environments, isolating Tseitin's transformation in both developments
and performing a more detailed comparison could be used to understand
the pros and cons of the two proof assistants (Dafny and Coq).

Verified transformation into CNF should be a first step in verified
SAT solvers that take as input arbitrary formulae. The SAT solver {\tt
  versat}~\cite{DBLP:conf/vmcai/OeSOC12} was implemented and verified
in the Guru programming language using dependent types. The solver is
verified to be sound: if it produces an {\tt UNSAT} answer, then the
input formula truly is unsatisfiable. Blanchette and
others~\cite{DBLP:journals/jar/BlanchetteFLW18} present a certified
SAT solving framework verified in the Isabelle/HOL proof
assistant. The proof effort is part of the \emph{Isabelle
  Formalization of Logic} project. The framework is based on
refinement: at the highest level sit several calculi like CDCL and
DPLL, which are formally proved. Depending on the strategy, the
calculi are also shown to be terminating. Another SAT solver verified
in Isabelle/HOL is by Mari\'{c}~\cite{DBLP:journals/jar/Maric09}. In
contrast to previous formalization, the verification methodology is
not based on refinement. Instead, the Hoare triples associated to the
solver pseudo-code are verified in Isabelle/HOL. In subsequent
work~\cite{DBLP:journals/corr/abs-1108-4368}, Mari{\'c} and Janičić
prove in Isabelle the functional correctness of a SAT solver
represented as an abstract transition system. Andrici and
Ciobâcă~\cite{DBLP:journals/corr/abs-2007-10842,DBLP:journals/corr/abs-1909-01743}
verify an implementation of DPLL in Dafny. Another formalization of a
SAT solver (extended with linear arithmetic) is by
Lescuyer~\cite{lescuyer:tel-00713668}, who verifies a DPLL-based
decision procedure for propositional logic in Coq and exposes it as a
reflexive tactic. Finally, a decision procedure based on DPLL is also
verified by Shankar and Vaucher~\cite{DBLP:journals/entcs/ShankarV11}
in the PVS system. For the proof, they rely on subtyping and dependent
types. Berger et al. have used the Minlog proof assistant to extract a
certified SAT solver~\cite{DBLP:journals/corr/BergerLFS15}.  {\bf None
  of the verified SAT solvers described above perform a CNF
  conversion}, with the exception of the reflexive procedure by
Lescuyer~\cite{lescuyer:tel-00713668}. Lescuyer notes that
implementing Tseitin's procedure in Coq proved to be \emph{much more
  challenging} and therefore implements a lazy CNF transformation.

\section{Discussion}
\label{sec:conclusion}

Compiling (including verification time) the entire development (the
two CNF transformations) takes about one minute on a standard
laptop. The following table contains a summary of the entire
development in numbers.

\begin{center}
\begin{tabular}{  l  r @{\hspace{2cm}} l r } 
\hline
Lines of code & 2440 & Methods & 32\\ 
%\hline
Preconditions & 318 & Postconditions & 176 \\
%\hline
Predicates & 26 & Functions & 26 \\
%\hline
Assertions & 227 & Lemmas & 51 \\
%\hline
Ghost variables & 31 & Verification time & $\sim$1 min \\
\hline
\end{tabular}
\end{center}

We find that Dafny can be used successfully to complete this case
study. However, the degree of proof automation is small and most of
the interesting verified proofs require significant assistance from
the user in the form of helper assertions and lemmas. Additionally, we
found that the verified proofs of the Tseitin algorithm push Dafny to
the edge, in the sense that a particular organization of the proofs is
required in order for the development to verify in reasonable time. To
this purpose, we propose the following \emph{verification patterns}
that help achieve good verification performance and that are portable
to other Dafny developments:

(1) Do not use nested quantifiers. Instead, whenever a formula like
\( \forall \_ . \exists \_. \_ \) occurs, create a predicate
\( Q \equiv \exists \_ . \_ \) and use \( \forall \_ . Q \) instead of
the initial formula. We use this verification pattern in the
verification of the Tseitin transformation in the context of the
\texttt{tseitinCanExtend} predicate. This transformation could be
automated and could serve as an improvement in deductive verifiers,
but further investigation into its merits on more case studies should
be performed first.

(2) Do \emph{not} inline even simple predicates. Make sure inlining is
consistent. For example, we prefer to use the following trivial
predicate:

\begin{dafny}
predicate validVariable(v : int) { v >= 0 }
\end{dafny}

\noindent instead of inlining it. Additionally, consistency is
required: mixing \texttt{v >= 0} and \texttt{validVariable(v)} will
generally result in a potential performance degradation in
verification % For example, if we replace in one of the project source
% files \texttt{validVariable(v1)} by \texttt{v1 >= 0} (10
% replacements), resulting in code like:
%
% \begin{dafny}
% [...]  requires v1 >= 0; // note mixed use of inlining
%        requires validVariable(v2); [...]
% \end{dafny}
%
% \noindent 
(e.g., mixing \texttt{v >= 0} and \texttt{validVariable(v)} in one
file results in a verification time increases by approx. 16\% in our project).

% Unfortunately, this is not an absolute recommendation. For example, we
% find that if we change the predicate \texttt{valid} as follows:

% \begin{dafny}
% predicate valid(f : FormulaT, rf : seq<seq<int> >,
%   v : int, n : int, start : int, end : int)
% {
%   0 <= n <= start <= end && variablesUpTo(f, n) && validCnfFormula(rf) &&
%   v >= 0 &&                  // <-- use this
%   // validVariable(v) &&   // <-- instead of this
%   variableInInterval(v, n, start, end) &&
%   variablesInInterval(rf, n, start, end)
% }
% \end{dafny}

% \noindent the verification time decreases to approx. 40s from
% approx. 1m. We suspect that this is because \texttt{v} is used both as
% a variable \emph{and} as an index. We prefer however to keep the more
% verbose version because it expresses the programmer intent better,
% even if verification performance is worse.

(3) Consistently add post-conditions, even if the they are
trivial. For example, if we remove the two post-conditions in the
following function method:

\begin{dafny}
function method negVarToLit(v : int) : int    requires validVariable(v);
  ensures negVarToLit(v) <= -1 && validLiteral(negVarToLit(v));
{ (-v) - 1 }
\end{dafny}

\noindent our development still verifies, but takes approx. 50\% more
time to do so (approx. 1m30s instead of approx. 1m). On more complex
functions, this can be the difference between verification succeeding
and failing (for no apparent reason).

Incidentally, the above patterns do not only help Dafny verify the
development faster, but also often clarify invariants for the
programmer by forcing them to consistently give names to certain
recurring formulae and enabling them to essentially create a mini-DSL
for proofs.

Our case study also suggests a few areas where Dafny and other similar
verifiers could be improved. For example, numerical functions such as
\texttt{pow} (exponentiation) could be built in, possibly with some
associated helper lemmas. Termination measures could be improved, both
in allowed syntax (e.g., allowing \texttt{decrease
  userdefinedfunction(...)}), but also in allowing other well-founded
orders such as multiset orderings (although we have finally not needed
such orders in our development). Another area that could be improved
is predictable verification performance. We find that, especially when
not following the patterns described above, performance is very
difficult to predict.%  Small changes, such as commenting out an
% assertion, could either improve verification time or make verification
% fail altogether. We think that verification time predictability should
% be improved, even to the detriment of automation.

% \emph{The Dafny development.} The attached Dafny development consists
% of 8 source files:

% \begin{enumerate*}

% \item \texttt{utils.dfy} contains generally useful definitions and
%   lemmas, such as the definition of exponentiation (\texttt{pow});

% \item \texttt{formula.dfy} contains the definition of the
%   \texttt{FormulaT} data type and related functions and lemmas such as
%   \texttt{validFormulaT}, \texttt{truthValue}, \texttt{maxVar};

% \item \texttt{cnf.dfy} contains the verified implementation of the
%   textbook algorithm for finding the CNF (with functions/methods such
%   as \texttt{convertToCNF} or \texttt{applyRule});

% \item \texttt{cnfformula.dfy} contains various items concerning the
%   representation of formulae as \texttt{seq<seq<int> >}, such as the
%   predicates \texttt{validLiteral}, \texttt{validCnfFormula},
%   \texttt{truthValueCnfFormula};

% \item \texttt{tseitin.dfy} contains the entry point (\texttt{tseitin})
%   to Tseitin's transformation, together with the main implementatino
%   \texttt{tseitinCnf};

% \item it relies on \texttt{tseitinproofs.dfy}, which contains lemmas
%   that prove the invariant of \texttt{tseitinCnf} for all cases;

% \item both of the modules above rely on \texttt{tseitincore.dfy},
%   which contains definitions useful in both the algorithm and its
%   proof, such as the set of clauses \texttt{orClauses} to be added for
%   disjunctions;

% \item \texttt{main.dfy} exercises the CNF transformation in a
%   \texttt{Main} method and can be used to obtain an executable.

% \end{enumerate*}

\emph{Implementation choices}. For Tseitin's algorithm, we use a
different representation for the input formula (an ADT) and the output
formula (a set of clauses). Not only is this representation of the
output the most natural for implementing the algorithm, but it is also
what a SAT solver takes as input. Therefore, it allows in principle to
easily combine our CNF transformation with a SAT solver. It would be
easy to convert the set of clauses into the ADT representation and
prove equivalence of the two, or to directly work with ADTs as part of
the transformation. For the textbook transformation, the output and
the input have the same representation (ADTs). However, we do not
prove explicitly that the result is in CNF (it follows implicitly from
the fact that none of the nine equivalences can be applied
anymore). Proving this explicitly would require to first specify what
is means for a formula to be in CNF; it would be an interesting
exercise to prove this and to extract the set of clauses from the CNF.

\emph{Future work}. Our case study opens several directions for future
work.
%
\emph{Possible improvements}. The formulae could be represented as
DAGs instead of trees. This could speed up both algorithms; more
interestingly, this might simplify the termination proofs for the
textbook algorithm as discussed in Difficulty D1 on
Page~\pageref{dif:d1}. Several optimizations to Tseitin's
algorithm~\cite{harrison,DBLP:journals/jsc/Tour92} could also be
implemented and verified. For a high-performance conversion, literals
and variables should be represented as machine integers, which would
require proving bounds throughout the code.
%
\emph{Theoretical extensions}. It would be interesting to find
improved (possibly tight) bounds on the termination measure for the
first algorithm. Additionally, it would be useful to bridge the
distance between \emph{truth assignment} as defined in the Dafny
development and the usual notion of \emph{truth assignment} in logic:
our assignments are finite (with sufficiently many elements to account
for all propositional variables in the context where they are used),
while \emph{truth assignments} are usually countably infinite.
%
\emph{Verification improvements}. It would be useful to further
simplify the variant used to prove termination for the first
algorithm. As Dafny supports a limited form of refinement
types~\cite{dafnyReferenceManual} (only numerical types can be refined
by a logical constraint), it might be useful for the verification time
to use such types for variables and literals.
%
\emph{Finally,} our CNF transformations can be linked with a verified
CNF-SAT solver to obtain an end-to-end verified solver for the general
SAT problem.

\clearpage

\bibliographystyle{plain}

\bibliography{refs}

\clearpage

\appendix

\section{Short Overview of Dafny}
\label{sec:dafny}

The Dafny programming language supports auto-active verification, a
technique known since the 70s, but which has only become practical in
the past decade, thanks to advances in both the usability of
verification tools and computer processing power. Here is an example
of a simple typical auto-actively verified Dafny method implementing
binary search:

\begin{dafny}
method binarySearch(a: array<int>, key : int) returns (r : int)
   requires forall j, k :: 0 <= j < k < a.Length ==> a[j] <= a[k];
   ensures r >= 0 ==> 0 <= r < a.Length && a[r] == key;
   ensures r < 0 ==> forall k :: 0 <= k < a.Length - 1 ==> a[k] != key;
{
   var left : int := 0;
   var right : int := a.Length - 1;
   while (left <= right)
      invariant 0 <= left <= a.Length;
      invariant -1 <= right < a.Length;
      invariant forall k :: 0 <= k < left ==> a[k] < key;
      invariant forall k :: right < k < a.Length ==> a[k] > key;
      decreases right - left;
   {
      var mid : int := (left + right) / 2;
      if (key < a[mid]) {
         right := mid - 1;
      } else if (key > a[mid]) {
         left := mid + 1;
      } else {
         return mid;
      }
   }
   return -1;
 }
\end{dafny}

Auto-active verification is a mix of \emph{auto}matic verification and
inter\emph{active} verification. It means that the program is
annotated by the programmer with specifications (such as
preconditions, introduced by \texttt{requires}, and postconditions,
introduced by \texttt{ensures}), which are automatically checked by
the system to hold. The system verifies that the program implements
the specification. If the postcondition cannot be proven to hold
whenever the precondition holds, then the Dafny compiler fails with an
error message. For example, the code above compiles (and verifies)
without any issues; however, if we had made any mistake, like
initializing \texttt{right} by \texttt{a.Length} instead of
\texttt{a.Length - 1} or reversing the comparison operators \texttt{<}
and \texttt{>} inside the body of the while loop, then compilation
would fail as the postcondition would not be provable.

For complicated post-conditions, Dafny cannot establish their validity
automatically, and therefore additional help is required from the part
of the programmer (the interactive part) in the form of invariants (in
the example above, four invariants are given for the while loop),
variants (introduced by the \texttt{decreases} keyword, used to
establish termination), lemmas, or other helper
annotations.

Auto-active verification is featured in frameworks such as
Why3~\cite{why3}, Dafny~\cite{DBLP:journals/software/Leino17} or even
programming languages such as Ada~\cite{DBLP:journals/sttt/HoangMWC15}
or C~\cite{DBLP:conf/tphol/CohenDHLMSST09}. It has been used
successfully to develop trusted code for small and even average sized
projects~\cite{DBLP:conf/osdi/HawblitzelHLNPZZ14}. The main advantage
of using auto-active verification in software development is that we
obtain a high degree of confidence in the correctness of software
projects that were developed in this style.

\section{The Dafny Development}
\label{sec:development}

The attached Dafny development consists of 8 source files:


\( \bullet \) \texttt{utils.dfy} contains generally useful definitions and
  lemmas, such as the definition of exponentiation (\texttt{pow});

\( \bullet \) \texttt{formula.dfy} contains the definition of the
  \texttt{FormulaT} data type and related functions and lemmas such as
  \texttt{validFormulaT}, \texttt{truthValue}, \texttt{maxVar};

\( \bullet \) \texttt{cnf.dfy} contains the verified implementation of the
  textbook algorithm for finding the CNF (with functions/methods such
  as \texttt{convertToCNF} or \texttt{applyRule});
  
\( \bullet \) \texttt{cnfformula.dfy} contains various items concerning the
  representation of CNF formulae as elements of type
  \texttt{seq<seq<int> >}, such as the predicates
  \texttt{validLiteral}, \texttt{validCnfFormula},
  \texttt{truthValueCnfFormula};

\( \bullet \) \texttt{tseitin.dfy} contains the entry point (\texttt{tseitin})
  to Tseitin's transformation, together with the main implementatino
  \texttt{tseitinCnf};

\( \bullet \) it relies on \texttt{tseitinproofs.dfy}, which contains lemmas
  that prove the invariant of \texttt{tseitinCnf} for all cases;

\( \bullet \) both of the modules above rely on \texttt{tseitincore.dfy},
  which contains definitions useful in both the algorithm and its
  proof, such as the set of clauses \texttt{orClauses} to be added for
  disjunctions;

\( \bullet \) \texttt{main.dfy} exercises the CNF transformation in a
  \texttt{Main} method and can be used to obtain an executable;

\( \bullet \) \texttt{Makefile} to be used in the usual Unix-like manner.

To compile (and verify) the development, it is sufficient to run:

\( \bullet \) \texttt{dafny /verifySeparately *.dfy}.

We have verified the source code with Dafny version 3.0.0.20820, but
some earlier versions should work as well.

The Dafny development is available at
\begin{center}\url{https://github.com/iordacheviorel/cnf-dafny}.\end{center}

\end{document}
